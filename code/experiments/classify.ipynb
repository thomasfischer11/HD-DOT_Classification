{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2444639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import configs as data_configs\n",
    "\n",
    "from configs import GlobalConfig, RunContext, syn_cfg, hd_sq_cfg, bsl_cfg, load_dataset_configs\n",
    "from classify_utils import (\n",
    "    within_subject_cv, parcel_within_subject_cv,\n",
    "    loso_classification, parcel_loso_classification,\n",
    "    extract_features, aggregate_runs, preload_runs,\n",
    "    subject_stats_to_df, build_save_dir, save_results_df,\n",
    "    apply_sma_pruning, load_sma_sens_channels, load_pickle\n",
    ")\n",
    "\n",
    "from plotting_utils import (\n",
    "    plot_grouped_bars_by_dt, plot_grouped_bars_by_subject,\n",
    "    loso_barplot, barplot_subsets, raincloud_subsets_runs\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Config\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "#data_types = ['BS_Laura', 'HD_Squeezing']\n",
    "#global_cfg = hd_sq_cfg\n",
    "data_types = ['HD_Squeezing']\n",
    "global_cfg = hd_sq_cfg\n",
    "#data_types = ['BS_Laura']\n",
    "#global_cfg = bsl_cfg\n",
    "\n",
    "#data_types = ['Syn_Finger_Tapping']\n",
    "#global_cfg = syn_cfg\n",
    "\n",
    "dataset_configs = load_dataset_configs(data_types, test=True, load_sensitivity=True)\n",
    "\n",
    "print(\"epo label path: \", dataset_configs[data_types[0]].epochs_labels_path(0,0,0,0))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Main Pipeline\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if global_cfg.run_pipeline:\n",
    "    for data_type in data_types:\n",
    "        ds = dataset_configs[data_type]\n",
    "        subjects, base_path = ds.subjects, ds.base_path\n",
    "        long_chs, probe_area = ds.long_channels, ds.probe_area\n",
    "        ft_slices = ds.feature_slices\n",
    "        result_path = os.path.join(global_cfg.result_root, data_type)\n",
    "\n",
    "        subsets_data = load_pickle(os.path.join(base_path, 'subsets_data'))\n",
    "        subset_keys = list(reversed(subsets_data.keys()))\n",
    "        #subset_keys = ['subset_3', 'subset_2', 'full']\n",
    "        channel_roi_laura = load_pickle(os.path.join(global_cfg.datasets_path, 'BS_Laura', \"BS_Laura_YY_parcel_sens_channels\"))\n",
    "\n",
    "        for int_scaling in global_cfg.int_scalings:\n",
    "            for spatial_scaling in global_cfg.spatial_scalings:\n",
    "                print(f\"\\nIntensity Scaling: {int_scaling} | Spatial Scaling: {spatial_scaling}\\n\")\n",
    "                data = None\n",
    "                data, clean_map = preload_runs(ds, subjects, base_path, int_scaling, spatial_scaling)\n",
    "\n",
    "                ctx = RunContext(\n",
    "                    g=global_cfg, ds=ds, feature_types=[\"Slope\"], reduce_features=False,\n",
    "                    prune_channels=True, prune_chans_sma=True, data_type=data_type,\n",
    "                    int_scaling=int_scaling, spatial_scaling=spatial_scaling,\n",
    "                    subsets_data=subsets_data, clean_ch_map=clean_map,\n",
    "                    sma_sens_channels= load_sma_sens_channels(base_path, data_type), ft_slices=ft_slices,\n",
    "                )\n",
    "\n",
    "                clean_sma_map = apply_sma_pruning(clean_map, ctx.sma_sens_channels) if ctx.prune_chans_sma else clean_map\n",
    "                #ctx.clean_ch_map = clean_sma_map\n",
    "\n",
    "                for clf_name, clf in global_cfg.classifiers.items():\n",
    "                    ctx.clf_name, ctx.classifier = clf_name, clf\n",
    "\n",
    "                    # ============== CHANNEL WS =========================\n",
    "                    ctx.prune_chans_sma = True\n",
    "                    if global_cfg.run_ch_ws:\n",
    "                        scores = {dt: {k: {} for k in subset_keys} for dt in global_cfg.dt_conditions}\n",
    "                        run_scores = {dt: {k: {} for k in subset_keys} for dt in global_cfg.dt_conditions}\n",
    "\n",
    "\n",
    "                        for dt in global_cfg.dt_conditions:\n",
    "                            ctx.dt = dt\n",
    "                            for sub_key in subset_keys:\n",
    "                                ctx.sub_key, subset_chs = sub_key, subsets_data[sub_key][\"all\"]\n",
    "\n",
    "                                for subject in subjects:\n",
    "                                    run_stats = []\n",
    "                                    for run in data[subject]:\n",
    "                                        ctx.subject, ctx.run = subject, run\n",
    "                                        if \"ss\" in dt:\n",
    "                                            run_data, ss_run_data = data[subject][run][sub_key][dt], None\n",
    "                                        else:\n",
    "                                            run_data = data[subject][run][\"all\"]\n",
    "                                            ss_run_data = data[subject][run][sub_key].get(dt + \"_ss_mean\")\n",
    "\n",
    "                                        prune_chs = clean_sma_map[subject][run] if ctx.prune_channels else None\n",
    "                                        run_stat = within_subject_cv(run_data, ss_run_data, ctx, subject, subset_chs, prune_chs)\n",
    "                                        run_stats.append(run_stat)\n",
    "\n",
    "                                        subj_stat = run_stat.get(subject, None)\n",
    "                                        if subj_stat is not None:\n",
    "                                            acc = subj_stat.get(\"mean\", np.nan)\n",
    "                                            if acc is not None and not np.isnan(acc):\n",
    "                                                run_scores[dt][sub_key].setdefault(subject, []).append(float(acc))\n",
    "\n",
    "                                    agg = aggregate_runs(run_stats)\n",
    "                                    scores[dt][sub_key][subject] = agg.get(subject, {\"mean\": np.nan, \"std\": np.nan})\n",
    "\n",
    "                                save_dir = build_save_dir(result_path, space=\"channel\", mode=\"ws\", ctx=ctx)\n",
    "                                os.makedirs(save_dir, exist_ok=True)\n",
    "                                df = subject_stats_to_df(scores[dt][sub_key], include_all_row=True)\n",
    "                                save_results_df(df, save_dir, f\"results_{dt}_{sub_key}.csv\")\n",
    "\n",
    "                        avg_by_dt, std_by_dt = {dt: [] for dt in global_cfg.dt_conditions}, {dt: [] for dt in global_cfg.dt_conditions}\n",
    "                        for dt in global_cfg.dt_conditions:\n",
    "                            for sub_key in subset_keys:\n",
    "                                vals = [scores[dt][sub_key][s][\"mean\"] for s in subjects if s in scores[dt][sub_key]]\n",
    "                                avg_by_dt[dt].append(np.nanmean(vals) if vals else np.nan)\n",
    "                                std_by_dt[dt].append(np.nanstd(vals) if vals else 0)\n",
    "\n",
    "                        optodes = [subsets_data[k][\"n_optodes\"] / probe_area for k in subset_keys]\n",
    "\n",
    "                        raincloud_subsets_runs(\n",
    "                            run_scores=run_scores,subset_keys=subset_keys,optodes_per_cm2=optodes,\n",
    "                            dt_labels=global_cfg.dt_labels,\n",
    "                            save_dir=build_save_dir(result_path, space=\"channel\", mode=\"ws_raincloud\", ctx=ctx),\n",
    "                            spatial_scaling=ctx.spatial_scaling, int_scaling=ctx.int_scaling,save_plot=global_cfg.save_plot,\n",
    "                        )\n",
    "\n",
    "                        #barplot_subsets(avg_by_dt, std_by_dt, subset_keys, optodes, global_cfg.dt_labels, save_dir,\n",
    "                        #                ctx.spatial_scaling, ctx.int_scaling, global_cfg.save_plot)\n",
    "\n",
    "                        full_scores = {dt: {s: scores[dt].get('full', {}).get(s, {}).get('mean', np.nan) for s in subjects}\n",
    "                                    for dt in global_cfg.dt_conditions}\n",
    "                        full_stds   = {dt: {s: scores[dt].get('full', {}).get(s, {}).get('std', np.nan) for s in subjects}\n",
    "                                    for dt in global_cfg.dt_conditions}\n",
    "\n",
    "                        plot_grouped_bars_by_dt(data_dict=full_scores, dt_labels_map=global_cfg.dt_labels, std_dict=full_stds, \n",
    "                                                title=\"Channel Space Classification Accuracy\")\n",
    "                        plot_grouped_bars_by_subject(data_dict=full_scores, dt_labels_map=global_cfg.dt_labels, std_dict=full_stds, \n",
    "                                                     title=\"Channel Space Classification Accuracy\")\n",
    "\n",
    "                    ctx.prune_chans_sma = False\n",
    "                    ctx.g.sel_hrf_roi = None\n",
    "                    # ============== CHANNEL LOSO =======================\n",
    "                    if global_cfg.run_ch_loso:\n",
    "                        for sub_key in [\"full\"]:\n",
    "                            ctx.sub_key = sub_key\n",
    "                            subset_chs = subsets_data[sub_key][\"all\"]\n",
    "                            for dt in global_cfg.dt_conditions:\n",
    "                                dt_before, dt_loso = dt, (dt + '_full') if 'ss' in dt else dt\n",
    "                                ctx.dt = dt\n",
    "                                all_data = {s: [] for s in subjects}\n",
    "                                for subject in subjects:\n",
    "                                    for run in data[subject]:\n",
    "                                        entry = data[subject][run]['full'][dt_loso] if 'ss' in dt_loso else data[subject][run]['all']\n",
    "                                        epo, y_raw = entry[\"epochs\"], entry[\"splits\"][0][\"y\"]\n",
    "                                        if ctx.data_type == 'BS_Laura':\n",
    "                                            epo = epo.sel(channel=[c for c in channel_roi_laura if c in epo.channel.values])\n",
    "                                        if ctx.dt == \"long\":\n",
    "                                            epo = epo.sel(channel=[c for c in long_chs if c in epo.channel.values])\n",
    "                                        chans = [c for c in subset_chs if c in epo.channel.values]\n",
    "                                        if ctx.prune_chans_sma and ctx.sma_sens_channels:\n",
    "                                            chans = [c for c in chans if c in ctx.sma_sens_channels]\n",
    "                                        epo = epo.sel(channel=chans)\n",
    "                                        X = extract_features(epo, ctx, long_chs=None, prune_chs=None)\n",
    "                                        if ctx.prune_channels and ctx.g.prune_by_zeroing_loso:\n",
    "                                            X = X.where(X.channel.isin(ctx.clean_ch_map[subject][run]), 0)\n",
    "                                        X_np = X.values if hasattr(X, \"values\") else X\n",
    "                                        y = LabelEncoder().fit_transform(y_raw)\n",
    "                                        all_data[subject].append((X_np, y))\n",
    "                                loso_stats = loso_classification(all_data, subjects, ctx, k=ctx.g.n_reduced_feat_loso)\n",
    "                                save_dir = build_save_dir(result_path, space=\"channel\", mode=\"loso\", ctx=ctx); os.makedirs(save_dir, exist_ok=True)\n",
    "                                ttl = f\"Channel LOSO per Subject - {global_cfg.dt_labels[dt_before]} - {ctx.clf_name} - subset: {sub_key}\"\n",
    "                                loso_barplot(loso_stats, ttl, save_dir,\n",
    "                                             f\"{ctx.clf_name.replace(' ', '_')}_loso_accuracy_{sub_key}_{dt_before}.png\",\n",
    "                                             global_cfg.save_plot)\n",
    "                                df = subject_stats_to_df(loso_stats, include_all_row=True)\n",
    "                                save_results_df(df, save_dir, f\"results_{dt_before}_{sub_key}.csv\")\n",
    "\n",
    "                    # ============== PARCEL WS ==========================\n",
    "                    if global_cfg.run_parcel_pipeline and global_cfg.run_parcel_ws:\n",
    "                        dt_parcel = [\"all_od\", \"all_od_ss_mean\"]\n",
    "                        dt_labels_parcel = {\"all_od\":\"Parcel No SS Correction\",\"all_od_ss_mean\":\"Parcel SS corrected\"}\n",
    "                        parcel_subset = ds.parcel_subset\n",
    "                        subset_name, parcels = list(parcel_subset.items())[0]\n",
    "\n",
    "                        for sub_key in subset_keys:\n",
    "                            ctx.sub_key = sub_key\n",
    "                            subset_chs = subsets_data[sub_key]['all']\n",
    "                            acc_by_dt, std_by_dt = {}, {}\n",
    "\n",
    "                            for dt in dt_parcel:\n",
    "                                ctx.dt = dt\n",
    "                                acc_by_dt[dt], std_by_dt[dt] = {}, {}\n",
    "                                for si, subject in enumerate(subjects):\n",
    "                                    run_stats = []\n",
    "                                    for run in range(ds.n_runs(si)):\n",
    "                                        run_data = data[subject][run][\"full\"][dt] if \"ss\" in dt else data[subject][run][\"all_od\"]\n",
    "                                        clean_parcel_chs = [c for c in clean_map[subject][run] if c in subset_chs]\n",
    "                                        stat = parcel_within_subject_cv(\n",
    "                                            run_data=run_data, ctx=ctx,\n",
    "                                            Adot=ds.Adot, B=ds.B,\n",
    "                                            clean_chs=clean_parcel_chs, parcels=parcels,\n",
    "                                            subject_id=subject, prune=ctx.prune_channels\n",
    "                                        ); run_stats.append(stat)\n",
    "                                    agg = aggregate_runs(run_stats)\n",
    "                                    acc_by_dt[dt][subject] = agg.get(subject, {}).get(\"mean\", np.nan)\n",
    "                                    std_by_dt[dt][subject] = agg.get(subject, {}).get(\"std\", 0.0)\n",
    "\n",
    "                                save_dir = build_save_dir(result_path, space=\"parcel\", mode=\"ws\", ctx=ctx)\n",
    "                                stats_dt = {s: {\"mean\": acc_by_dt[dt].get(s, np.nan), \"std\": std_by_dt[dt].get(s, np.nan)} for s in subjects}\n",
    "                                df = subject_stats_to_df(stats_dt, include_all_row=True)\n",
    "                                save_results_df(df, save_dir, f\"results_{dt}_{subset_name}_chset_{sub_key}.csv\")\n",
    "\n",
    "                            plot_grouped_bars_by_dt(data_dict=acc_by_dt, dt_labels_map=dt_labels_parcel, std_dict=std_by_dt,\n",
    "                                                    title=f\"Parcel Space WS Accuracy {ctx.data_type} (ch subset: {sub_key})\")\n",
    "                            plot_grouped_bars_by_subject(data_dict=acc_by_dt, dt_labels_map=dt_labels_parcel, std_dict=std_by_dt,\n",
    "                                                         title=f\"Parcel Space WS Accuracy {ctx.data_type} (ch subset: {sub_key})\")\n",
    "\n",
    "                    # ============== PARCEL LOSO ========================\n",
    "                    if global_cfg.run_parcel_pipeline and global_cfg.run_parcel_loso:\n",
    "                        dt_parcel = [\"all_od\", \"all_od_ss_mean\"]\n",
    "                        dt_labels_parcel = {\"all_od\":\"Parcel No SS Correction\",\"all_od_ss_mean\":\"Parcel SS corrected\"}\n",
    "                        parcel_subset = {\"sensitive\": ds.sensitive_parcels}\n",
    "                        subset_name, parcels = list(parcel_subset.items())[0]\n",
    "\n",
    "                        for sub_key in [\"full\"]:\n",
    "                            ctx.sub_key = sub_key\n",
    "                            subset_chs = subsets_data[sub_key]['all']\n",
    "                            for dt in dt_parcel:\n",
    "                                dt_before, dt_loso = dt, (dt + '_full') if 'ss' in dt else dt\n",
    "                                data_parcel = {\n",
    "                                    s: {r: (data[s][r]['full'][dt_loso] if \"ss\" in dt_loso else data[s][r]['all_od']) for r in data[s]}\n",
    "                                    for s in data\n",
    "                                }\n",
    "                                clean_sub = {\n",
    "                                    sbj: {run: [\n",
    "                                        c for c in clean_map[sbj][run] if c in subset_chs\n",
    "                                    ] for run in clean_map[sbj]} for sbj in clean_map\n",
    "                                }\n",
    "                                loso_stats = parcel_loso_classification(\n",
    "                                    data=data_parcel, subjects=subjects, ctx=ctx,\n",
    "                                    Adot=ds.Adot, B=ds.B, clean_ch_map=clean_sub,\n",
    "                                    parcels=parcels, prune=ctx.prune_channels\n",
    "                                )\n",
    "                                save_dir = build_save_dir(result_path, space=\"parcel\", mode=\"loso\", ctx=ctx); os.makedirs(save_dir, exist_ok=True)\n",
    "                                ttl = f\"Parcel LOSO per Subject - {dt_labels_parcel[dt_before]} - {ctx.clf_name} - ch subset: {sub_key}\"\n",
    "                                loso_barplot(loso_stats, ttl, save_dir,\n",
    "                                             f\"{ctx.clf_name.replace(' ', '_')}_loso_accuracy_{sub_key}_{dt_before}.png\",\n",
    "                                             global_cfg.save_plot)\n",
    "                                df = subject_stats_to_df(loso_stats, include_all_row=True)\n",
    "                                save_results_df(df, save_dir, f\"results_{dt_before}_{subset_name}_chset_{sub_key}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedalion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
