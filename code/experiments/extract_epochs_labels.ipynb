{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cedalion\n",
    "import cedalion.nirs\n",
    "import cedalion.datasets\n",
    "import cedalion.plots\n",
    "import xarray as xr\n",
    "from cedalion import units\n",
    "import cedalion.models.glm as glm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cedalion.sigproc.motion_correct as motion_correct\n",
    "from cedalion.sigproc.quality import repair_amp\n",
    "import cedalion.models.glm.design_matrix as glm_dm\n",
    "import configs as data_configs\n",
    "\n",
    "xr.set_options(display_expand_data=False);\n",
    "path_prefix = data_configs.data_path_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cv_splits(stim_df, n_splits):\n",
    "    y = stim_df['trial_type']\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    splits = []\n",
    "    for train_index, test_index in skf.split(np.zeros(len(y)), y):\n",
    "        train_stim = stim_df.iloc[train_index]\n",
    "        test_stim = stim_df.iloc[test_index]\n",
    "        splits.append({'train_stim': train_stim, 'test_stim': test_stim})\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_glm_excluding_test(ts, test_stim, dm, mask_before_secs, mask_after_secs, method='individual', noise_model='ols'):\n",
    "\n",
    "    dm_modified = dm.common.copy()\n",
    "\n",
    "    channel_wise_regs_modified = None\n",
    "\n",
    "    if dm.channel_wise:\n",
    "        channel_wise_regs_modified = [dm.channel_wise[0].copy()]\n",
    "        unit = dm.channel_wise[0].pint.units\n",
    "    else:\n",
    "        channel_wise_regs_modified = []\n",
    "\n",
    "    # Old Method: Zero out the design matrix from before first test stimulus to after last test stimulus\n",
    "    # Problem: There can be train stims in between that get zeroed out, if the trial types don't always occur alternating\n",
    "    if method == 'block':\n",
    "        # Identify the earliest and latest test stimulus onset times\n",
    "        te_0 = test_stim['onset'].min()\n",
    "        te_1 = test_stim['onset'].max()\n",
    "        duration = test_stim.loc[test_stim['onset'] == te_1, 'duration'].values[0]\n",
    "\n",
    "        # Define the time window to exclude (test data plus epochs around stimuli)\n",
    "        exclude_start = te_0 - mask_before_secs  \n",
    "        exclude_end = te_1 + duration + mask_after_secs   \n",
    "\n",
    "        # Zero out the design matrix rows corresponding to the test data timepoints\n",
    "        mask = (dm_modified['time'] >= exclude_start) & (dm_modified['time'] <= exclude_end)\n",
    "        dm_modified = dm_modified.where(~mask, other=0)\n",
    "        if dm.channel_wise:\n",
    "            channel_wise_regs_modified[0] = channel_wise_regs_modified[0].where(~mask, other=0 * unit)\n",
    "\n",
    "\n",
    "    # New Method: Zero out the design matrix around each test stimulus.\n",
    "    if method == 'individual':\n",
    "        for _, stim in test_stim.iterrows():\n",
    "            onset = stim['onset']\n",
    "            duration = stim['duration']\n",
    "            # Define mask from onset to onset + duration\n",
    "            mask = (dm_modified['time'] >= onset - mask_before_secs) & (dm_modified['time'] <= onset + duration + mask_after_secs)\n",
    "            #mask = xr.DataArray(mask, dims='time', coords={'time': dm['time']})\n",
    "            dm_modified = dm_modified.where(~mask, other=0)\n",
    "            if dm.channel_wise:\n",
    "                channel_wise_regs_modified[0] = channel_wise_regs_modified[0].where(~mask, other=0 * unit)\n",
    "\n",
    "    \n",
    "    updated_dm = glm_dm.DesignMatrix(\n",
    "        dm_modified,\n",
    "        channel_wise_regs_modified,\n",
    "    )\n",
    "\n",
    "    # Fit the GLM using the modified design matrix\n",
    "    reg_results = glm.fit(ts, updated_dm, noise_model=noise_model)\n",
    "    reg_results = reg_results.sm.params\n",
    "    #print(\"reg_results: \", reg_results)\n",
    "    return reg_results, updated_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_epochs_labels(ts, stim_df, cv_splits, trial_types, before_secs, max_stim_dur, after_secs, dm=None, exclude_test=True, glm_test_method='individual', noise_model='ols'):\n",
    "    split_metadata = []\n",
    "    epochs_list = []\n",
    "    epo_after_secs = max_stim_dur + after_secs\n",
    "\n",
    "    use_shared_epochs = (dm is None) or (exclude_test is False)\n",
    "\n",
    "    print(\"Using shared epochs:\", use_shared_epochs)\n",
    "    print(\"dm:\", dm)\n",
    "\n",
    "    # Precompute GLM + epochs once if applicable\n",
    "    if use_shared_epochs:\n",
    "        ts_data = ts.copy()\n",
    "\n",
    "        if dm is not None:\n",
    "            reg_results = glm.fit(ts_data, dm, noise_model=noise_model)\n",
    "            betas = reg_results.sm.params\n",
    "\n",
    "            pred_wo_hrf = glm.predict(ts, betas.sel(regressor=~betas.regressor.str.startswith(\"HRF \")), dm)\n",
    "\n",
    "            if ts_data.pint.units == cedalion.units.micromolar:\n",
    "                pred_wo_hrf = pred_wo_hrf.pint.quantify(\"micromolar\")\n",
    "\n",
    "            ts_data = ts_data - pred_wo_hrf\n",
    "\n",
    "        # Extract epochs once\n",
    "        shared_epochs = ts_data.cd.to_epochs(\n",
    "            stim_df,\n",
    "            trial_types,\n",
    "            before=before_secs * units.seconds,\n",
    "            after=epo_after_secs * units.seconds\n",
    "        )\n",
    "\n",
    "        baseline = shared_epochs.sel(reltime=(shared_epochs.reltime < 0)).mean(\"reltime\")\n",
    "        shared_epochs = shared_epochs - baseline\n",
    "\n",
    "    for split in cv_splits:\n",
    "        train_stim = split['train_stim']\n",
    "        test_stim = split['test_stim']\n",
    "\n",
    "        diff_hrf = None\n",
    "        hrf_diff_sorted = None\n",
    "\n",
    "        if use_shared_epochs:\n",
    "            epochs = shared_epochs\n",
    "        else:\n",
    "            ts_data = ts.copy()\n",
    "\n",
    "            # Fit GLM with test-exclusion\n",
    "            betas, dm_modified = fit_glm_excluding_test(ts_data, test_stim, dm, before_secs, after_secs, method=glm_test_method, noise_model=noise_model)\n",
    "\n",
    "            pred_wo_hrf = glm.predict(ts, betas.sel(regressor=~betas.regressor.str.startswith(\"HRF \")), dm)\n",
    "\n",
    "            if ts_data.pint.units == cedalion.units.micromolar:\n",
    "                pred_wo_hrf = pred_wo_hrf.pint.quantify(\"micromolar\")\n",
    "\n",
    "            # debug plots\n",
    "            #if 'chromo' in ts_data.dims:\n",
    "                # plot ts data before and after removing prediction without HRF (for random channel)\n",
    "                #print(\"ts data: \", ts_data)\n",
    "\n",
    "                #rch = 10\n",
    "                #plt.figure(figsize=(12, 6))\n",
    "                #plt.plot(ts_data.sel(time=slice(0,100)).time.values , ts_data.sel(chromo='HbO', time=slice(0,100)).isel(channel=rch).values, label='Original TS Data', alpha=0.7)\n",
    "                #plt.plot(ts_data.sel(time=slice(0,100)).time.values, pred_wo_hrf.sel(chromo='HbO', time=slice(0,100)).isel(channel=rch).values, label='Predicted without HRF', alpha=0.7)\n",
    "                #plt.plot(ts_data.sel(time=slice(0,100)).time.values, (ts_data - pred_wo_hrf).sel(chromo='HbO', time=slice(0,100)).isel(channel=rch).values, label='TS Data after Removing Prediction without HRF', alpha=0.7)\n",
    "                #pred_w_hrf = glm.predict(ts, betas.sel(regressor=betas.regressor.str.startswith(\"HRF \")), dm)\n",
    "                #plt.plot(ts_data.sel(time=slice(0,100)).time.values, pred_w_hrf.sel(chromo='HbO', time=slice(0,100)).isel(channel=rch).values, label='Predicted with HRF', alpha=0.7)\n",
    "                #for _, stim in stim_df.iterrows():\n",
    "                #    if stim['onset'] < 100:\n",
    "                #        plt.axvline(x=stim['onset'], color='red', linestyle='--', alpha=0.5)\n",
    "                #plt.xlabel('Time (s)')\n",
    "                #plt.ylabel('HbO Concentration (micromolar)')\n",
    "                #plt.title(f'Channel: {ts_data.channel.values[rch]}')\n",
    "                #plt.legend()\n",
    "                #plt.show()\n",
    "\n",
    "            ts_data = ts_data - pred_wo_hrf\n",
    "\n",
    "            # Per-split epoch extraction\n",
    "            epochs = ts_data.cd.to_epochs(\n",
    "                stim_df,\n",
    "                trial_types,\n",
    "                before=before_secs * units.seconds,\n",
    "                after=epo_after_secs * units.seconds\n",
    "            )\n",
    "\n",
    "            baseline = epochs.sel(reltime=(epochs.reltime < 0)).mean(\"reltime\")\n",
    "            epochs = epochs - baseline\n",
    "\n",
    "            regs = dm.common.regressor\n",
    "            if 'chromo' in dm.common.dims:\n",
    "                hrf_regs = regs.sel(regressor=regs.regressor.str.startswith(\"HRF\")).values\n",
    "                if hrf_regs.size > 2:\n",
    "                    # Select the HRF weights for the two trial types\n",
    "                    hrf1 = betas.sel(regressor=betas.regressor.str.startswith(\"HRF \" + trial_types[0]), chromo='HbO')\n",
    "                    hrf2 = betas.sel(regressor=betas.regressor.str.startswith(\"HRF \" + trial_types[1]), chromo='HbO')\n",
    "                    # Square differences and sum over 'regressor' to get squared L2 norm per channel\n",
    "                    diff_hrf = ((hrf1 - hrf2) ** 2).sum(dim='regressor')\n",
    "                    #diff_hrf = (betas.sel(regressor=betas.regressor.str.startswith(\"HRF \" + trial_types[0]), chromo='HbO') - (betas.sel(regressor=betas.regressor.str.startswith(\"HRF \" + trial_types[1]), chromo='HbO')))\n",
    "                else:\n",
    "                    diff_hrf = betas.sel(regressor=hrf_regs[0], chromo='HbO') - betas.sel(regressor=hrf_regs[1], chromo='HbO')\n",
    "                sorted_indices = np.argsort(diff_hrf.values)[::-1]\n",
    "                hrf_diff_sorted = list(diff_hrf.channel[sorted_indices].values)\n",
    "\n",
    "\n",
    "        split_metadata.append({\n",
    "            'train_indices': train_stim.index.values,\n",
    "            'test_indices': test_stim.index.values,\n",
    "            'y': epochs['trial_type'].values,\n",
    "            'hrf_diff': diff_hrf\n",
    "        })\n",
    "\n",
    "        if not use_shared_epochs:\n",
    "            epochs_list.append(epochs)\n",
    "\n",
    "    epochs_labels = {\n",
    "        'epochs': shared_epochs if use_shared_epochs else epochs_list,\n",
    "        'splits': split_metadata\n",
    "    }\n",
    "\n",
    "    return epochs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_vs_rest = True\n",
    "save = True\n",
    "test = True\n",
    "before_secs = 2\n",
    "after_secs = 8\n",
    "dataset_configs = data_configs.load_dataset_configs([\"HD_Squeezing\", \"Syn_Finger_Tapping\", \"BS_Laura\"], load_sensitivity=False, test=test)\n",
    "dist_thresh = 2.6 # cm threshold for short vs long channels\n",
    "\n",
    "print(\"SAVE = \", save)\n",
    "bs_laura_only_roi = True\n",
    "if bs_laura_only_roi:\n",
    "    with open(path_prefix + 'BS_Laura/BS_Laura_YY_parcel_sens_channels', 'rb') as f:\n",
    "        channel_roi_bs_laura = pickle.load(f)\n",
    "    print(\"len channel_roi_laura: \", len(channel_roi_bs_laura))\n",
    "\n",
    "\n",
    "for data_type in ['Syn_Finger_Tapping']:\n",
    "    \n",
    "    synthetic = data_type.startswith(\"Syn\")\n",
    "    spatial_scales = [1, 2, 3] if synthetic else [1]\n",
    "\n",
    "    for spatial_scaling in spatial_scales:\n",
    "\n",
    "        glm_test_m = 'block' if synthetic else 'individual'\n",
    "        intensity_keys = [\"\", \"_02\", \"_03\"] if synthetic else [\"\"]\n",
    "\n",
    "        # Load data\n",
    "        recs = {}\n",
    "        runs = {}\n",
    "\n",
    "        config = dataset_configs.get(data_type)\n",
    "        if config is None:\n",
    "            raise ValueError(f\"Unknown data_type: {data_type}\")\n",
    "        \n",
    "        base_path = config.base_path\n",
    "\n",
    "        subjects = config.subjects\n",
    "        with open(base_path + config.subsets_path, 'rb') as file:\n",
    "            subsets_data = pickle.load(file)\n",
    "        subset_keys = list(subsets_data.keys())\n",
    "\n",
    "        print(\"SUBSET KEYS:\")\n",
    "        print(subset_keys)\n",
    "\n",
    "        print(\"SUBJECTS:\", subjects)\n",
    "        for subject in subjects:\n",
    "\n",
    "            print(\"\")\n",
    "            print(f\"Subject {subject}\")\n",
    "            print(\"\")\n",
    "\n",
    "            run_list = config.runs(subject)\n",
    "            print(subject)\n",
    "            recs[subject] = []\n",
    "\n",
    "            for run_idx, run in enumerate(run_list):\n",
    "\n",
    "                print(\"\")\n",
    "                print(f\"Run {run_idx}\")\n",
    "                print(\"\")\n",
    "\n",
    "                path = config.snirf_path.format(subject=subject, run=run, spatial_scale=spatial_scaling)\n",
    "\n",
    "                clean_chs_path = config.clean_channels_path(subject=subject, run=run_idx)\n",
    "\n",
    "                clean_chs_full_path = base_path + clean_chs_path\n",
    "                \n",
    "                with open(clean_chs_full_path, 'rb') as f:\n",
    "                    clean_channels = pickle.load(f)\n",
    "\n",
    "                print(len(clean_channels))\n",
    "\n",
    "                rec = cedalion.io.read_snirf(base_path + path)[0]\n",
    "\n",
    "                print(\"PATH\", base_path + path)\n",
    "\n",
    "                dpf = xr.DataArray(\n",
    "                    [6, 6],\n",
    "                    dims=\"wavelength\",\n",
    "                    coords={\"wavelength\": rec[\"amp\"].wavelength},\n",
    "                )\n",
    "\n",
    "                # Stimulus processing\n",
    "                stim_df = rec.stim.copy()\n",
    "\n",
    "                if data_type == 'BS_Laura':\n",
    "                    stim_path = config.stim_template.format(subject=subject, run=run)\n",
    "                    stim_df = pd.read_csv(base_path + stim_path, sep='\\t')\n",
    "\n",
    "                if config.preprocess_stim is not None:\n",
    "                    stim_df = config.preprocess_stim(stim_df)\n",
    "                    if not stim_df.empty and \"onset\" in stim_df.columns:\n",
    "                        stim_df = stim_df.reset_index(drop=True)\n",
    "\n",
    "                # Add rest vs stim if needed\n",
    "                #if config.get(\"add_rest\", False) and stim_vs_rest:\n",
    "                #    df = stim_df.copy()\n",
    "                #    df['trial_type'] = 'stim'\n",
    "                #    rest_df = df.copy()\n",
    "                #    rest_df['onset'] = rest_df['onset'] + 20\n",
    "                #    rest_df['trial_type'] = 'rest'\n",
    "                #    rest_df['duration'] = 7\n",
    "                #    df_combined = pd.concat([df, rest_df], ignore_index=True).sort_values(by='onset').reset_index(drop=True)\n",
    "                #    df_combined = df_combined[df_combined['onset'] != df_combined['onset'].iloc[-1]]\n",
    "                #    stim_df = df_combined\n",
    "\n",
    "                for int_key in intensity_keys:\n",
    "                    \n",
    "                    print(f\"Processing intensity: '{int_key or 'base'}'\")\n",
    "\n",
    "                    if synthetic:\n",
    "                        od = rec[\"od\" + int_key]\n",
    "                    elif data_type == 'BS_Laura':\n",
    "                        rec[\"amp\"] = repair_amp(rec[\"amp\"], median_len=0)\n",
    "                        od = cedalion.nirs.int2od(rec['amp'])\n",
    "                    else:\n",
    "                        rec[\"amp\"] = repair_amp(rec[\"amp\"])\n",
    "                        od = cedalion.nirs.int2od(rec['amp'])\n",
    "                    if bs_laura_only_roi and data_type == 'BS_Laura':\n",
    "                        print(\"OD BEFORE\")\n",
    "                        print(od.shape)\n",
    "                        od = od.sel(channel=channel_roi_bs_laura)\n",
    "                        print(\"OD AFTER\")\n",
    "                        print(od.shape)\n",
    "                    od = motion_correct.tddr(od)\n",
    "                    od = motion_correct.motion_correct_wavelet(od)\n",
    "                    od_bp = od.cd.freq_filter(fmin=0.02, fmax=0.5, butter_order=4)\n",
    "                    od_hp = od.cd.freq_filter(fmin=0.02, fmax=0, butter_order=4)\n",
    "                    dpf = xr.DataArray([6, 6], dims=\"wavelength\", coords={\"wavelength\": rec[\"amp\"].wavelength})\n",
    "                    conc_bp = cedalion.nirs.od2conc(od_bp, rec.geo3d, dpf, spectrum=\"prahl\")\n",
    "                    conc_hp = cedalion.nirs.od2conc(od_hp, rec.geo3d, dpf, spectrum=\"prahl\")\n",
    "\n",
    "                    splits = create_cv_splits(stim_df, 5)\n",
    "\n",
    "                    trial_types = list(set(stim_df.trial_type.values))\n",
    "                    max_stim_dur = round(stim_df.duration.max())\n",
    "                    print(stim_df)\n",
    "                    print(\"max stim dur: \", max_stim_dur)\n",
    "\n",
    "                    # Band-passed data is the 'default'.\n",
    "                    od = od_bp\n",
    "                    conc = conc_bp\n",
    "\n",
    "                    epochs_labels_data = {}\n",
    "\n",
    "                    print(\"\")\n",
    "                    print(\"STIM DF\")\n",
    "                    print(stim_df)\n",
    "                    print(\"\")\n",
    "\n",
    "                    # Extract sparse subsets\n",
    "                    for sub_key in subset_keys:\n",
    "                \n",
    "                        # Select sparse subset channels\n",
    "                        subset_channels = subsets_data[sub_key]['all']\n",
    "\n",
    "                        print(\"Subset size: \", len(subset_channels))\n",
    "                        print(\"OD Size\", od.channel.values.size)\n",
    "\n",
    "                        od = od.sel(channel=[c for c in od.channel.values if c in subset_channels])\n",
    "                        od_hp = od_hp.sel(channel=[c for c in od_hp.channel.values if c in subset_channels])\n",
    "                        conc = conc.sel(channel=[c for c in conc.channel.values if c in subset_channels])\n",
    "                        conc_hp = conc_hp.sel(channel=[c for c in conc_hp.channel.values if c in subset_channels])\n",
    "\n",
    "                        ts_long, ts_short = cedalion.nirs.split_long_short_channels(\n",
    "                            conc, rec.geo3d, distance_threshold=dist_thresh * units.cm\n",
    "                        )\n",
    "\n",
    "                        conc_clean = conc.sel(channel=[c for c in conc.channel.values if c in clean_channels])\n",
    "                        od_clean = od.sel(channel=[c for c in conc.channel.values if c in clean_channels])\n",
    "\n",
    "                        ts_short_clean = ts_short.sel(channel=[c for c in ts_short.channel.values if c in clean_channels])\n",
    "                        ts_long_clean = ts_long.sel(channel=[c for c in ts_long.channel.values if c in clean_channels])\n",
    "\n",
    "                        ts_long_hp, ts_short_hp = cedalion.nirs.split_long_short_channels(\n",
    "                            conc_hp, rec.geo3d, distance_threshold=dist_thresh * units.cm\n",
    "                        )\n",
    "                        ts_short_hp_clean = ts_short_hp.sel(channel=[c for c in ts_short_hp.channel.values if c in clean_channels])\n",
    "                        ts_long_hp_clean = ts_long_hp.sel(channel=[c for c in ts_long_hp.channel.values if c in clean_channels])\n",
    "\n",
    "                        ts_long_od, ts_short_od = cedalion.nirs.split_long_short_channels(\n",
    "                            od, rec.geo3d, distance_threshold=dist_thresh * units.cm\n",
    "                        )\n",
    "                        ts_short_od_clean = ts_short_od.sel(channel=[c for c in ts_short_od.channel.values if c in clean_channels])\n",
    "\n",
    "                        ss_reg = ts_short_clean\n",
    "                        ss_reg_od = ts_short_od_clean\n",
    "                        if sub_key == 'subset_3':\n",
    "                            # special case for sparsest subset since we only have very few or no channels:\n",
    "                            # just take mean signal from all channels as proxy for physiological noise\n",
    "                            ss_reg = conc_clean\n",
    "                            ss_reg_od = od_clean\n",
    "\n",
    "                        sigma_val = 3\n",
    "                        T_val = 2\n",
    "                    \n",
    "                        # Design matrices\n",
    "\n",
    "                        # we only need this data once (we can select sparse subset channels later) (saves memory if we only save it once for all)\n",
    "                        if sub_key == 'full':\n",
    "                            epochs_labels_data['all'] = prepare_epochs_labels(conc, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs)\n",
    "                            if not data_type.startswith(\"Syn\"):\n",
    "                                epochs_labels_data['all_od'] = prepare_epochs_labels(od, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs)\n",
    "\n",
    "                        \n",
    "                        dm_mean = (\n",
    "                            glm.design_matrix.hrf_regressors(\n",
    "                                ts_long, stim_df, glm.Gamma(tau=0 * units.s, sigma=sigma_val * units.s, T=T_val * units.s)\n",
    "                            )\n",
    "                            & glm.design_matrix.drift_regressors(ts_long, drift_order=1)\n",
    "                            & glm.design_matrix.average_short_channel_regressor(ss_reg)\n",
    "                        )\n",
    "\n",
    "                        dm_mean_loso = (\n",
    "                            glm.design_matrix.drift_regressors(ts_long, drift_order=1)\n",
    "                            & glm.design_matrix.average_short_channel_regressor(ss_reg)\n",
    "                        )\n",
    "\n",
    "                        dm_mean_od = (\n",
    "                            glm.design_matrix.hrf_regressors(\n",
    "                                od, stim_df, glm.Gamma(tau=0 * units.s, sigma=sigma_val * units.s, T=T_val * units.s)\n",
    "                            )\n",
    "                            & glm.design_matrix.drift_regressors(od, drift_order=1)\n",
    "                            & glm.design_matrix.average_short_channel_regressor(ss_reg_od)\n",
    "                        )\n",
    "\n",
    "                        dm_mean_od_loso = (\n",
    "                            glm.design_matrix.drift_regressors(od, drift_order=1)\n",
    "                            & glm.design_matrix.average_short_channel_regressor(ss_reg_od)\n",
    "                        )\n",
    "\n",
    "                        # Extract epochs & labels for different data & design matrices\n",
    "\n",
    "                        epochs_labels_data[sub_key] = {}\n",
    "\n",
    "                        # the ss-regressed data differs for each subset\n",
    "                        #print(\"all_od_ss_mean\")\n",
    "\n",
    "                        #print(\"long_ss_mean\")\n",
    "                        epochs_labels_data[sub_key]['long_ss_mean'] = prepare_epochs_labels(ts_long, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean, glm_test_method=glm_test_m)\n",
    "                        epochs_labels_data[sub_key]['all_ss_mean'] = prepare_epochs_labels(conc, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean, glm_test_method=glm_test_m)\n",
    "\n",
    "                        if not data_type.startswith(\"Syn\"):\n",
    "                            epochs_labels_data[sub_key]['all_od_ss_mean'] = prepare_epochs_labels(od, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean_od, glm_test_method=glm_test_m)\n",
    "                            epochs_labels_data[sub_key]['all_od_ss_mean_full'] = prepare_epochs_labels(od, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean_od_loso, glm_test_method=glm_test_m, exclude_test=False)\n",
    "                            epochs_labels_data[sub_key]['all_ss_mean_full'] = prepare_epochs_labels(conc, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean_loso, glm_test_method=glm_test_m, exclude_test=False)\n",
    "                        #epochs_labels_data['long_ss_corr'] = prepare_epochs_labels(ts_long, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_corr, glm_test_method=glm_test_m)\n",
    "                        #epochs_labels_data['long_ss_corr_full'] = prepare_epochs_labels(ts_long, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_corr_loso, glm_test_method=glm_test_m, exclude_test=False)\n",
    "                        #epochs_labels_data['long_ss_corr_ar'] = prepare_epochs_labels(ts_long_hp, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean_hp, glm_test_method=glm_test_m, noise_model='ar_irls')\n",
    "                        #epochs_labels_data['long_ss_corr_ar_full'] = prepare_epochs_labels(ts_long_hp, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean_hp_loso, glm_test_method=glm_test_m, noise_model='ar_irls', exclude_test=False)\n",
    "                        #epochs_labels_data['long_long_mean'] = prepare_epochs_labels(ts_long, stim_df, splits, trial_types, before_secs, max_stim_dur, after_secs, dm=dm_mean_long, glm_test_method=glm_test_m)\n",
    "                        \n",
    "                    print(\"------------------------------------\")\n",
    "                    print(\"END KEYS\")\n",
    "                    print(\"------------------------------------\")\n",
    "                    print(epochs_labels_data.keys())\n",
    "                    # Save results\n",
    "                    if save:\n",
    "                        intensity_folder = \"\"\n",
    "                        if synthetic:\n",
    "                            intensity_folder = \"01\" if int_key == \"\" else int_key.lstrip(\"_\")\n",
    "                        epo_label_path = config.epochs_labels_path(subject=subject, run=run_idx, int_scaling=intensity_folder, spatial_scaling=spatial_scaling)\n",
    "                        full_output_path = os.path.join(base_path, epo_label_path)\n",
    "                        print(\"Saving to: \", full_output_path)\n",
    "                        os.makedirs(os.path.dirname(full_output_path), exist_ok=True)\n",
    "\n",
    "\n",
    "                        #filename = f\"run{run_idx}_epochs_labels.pkl\"\n",
    "                        with open(full_output_path, 'wb') as f:\n",
    "                            pickle.dump(epochs_labels_data, f)\n",
    "\n",
    "                        print(f\"Saved: {full_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedalion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
